---
title: "N√•r 'ingen effekt' egentlig betyr 'vi har ikke peiling'"
image: 1.png
format:
  html:
    embed-resources: true
---

```{r}
#| echo: false
#| warning: false
library(patchwork)
library(tidyverse)
library(MASS)

set.seed(123)

colorPalette <- c("#DD1E88FF", "#1C8EB3FF", "#8FC344FF") 
theme_sim <- ggplot2::theme_minimal() %+replace% ggplot2::theme(
  axis.text.y = element_text(size = 12),
  axis.text.x = element_text(size = 12),
  axis.title = element_text(size = 14),
  strip.text = element_text(size = 11, color = "black"),
  strip.background = element_blank(),
  plot.title = element_text(size = 16),
  panel.grid = element_blank(),
  panel.border = element_blank(),
  axis.line.x = element_line(),
  axis.line.y = element_line(),
  legend.position = "bottom",
  legend.key.height = unit(0.2, 'cm'),
  legend.key.width = unit(0.4, 'cm'),
  legend.title = element_text(size=14),
  legend.text = element_text(size=10),
  legend.box.background = element_blank(),
  legend.key = element_rect(fill = NA, color = NA))




```

De siste √•rene har vi sett en rekke forskningsstudier med konklusjoner som "ingen effekt av X p√• Y" eller "A gir samme effekt som B p√• Y". Et ferskt eksempel er Norges idrettsh√∏gskole, som sl√•r fast at "leik gir samme effekt som intervall". Slike konklusjoner kan virke underlige og st√•r ofte i klar kontrast til treneres erfaringer. Men det er ikke poenget her. Poenget er at slike konklusjoner ikke er logisk gyldige uten et eksperimentelt design som faktisk tillater dem, noe de f√¶rreste studier har. Likevel presenteres resultatene ofte som skr√•sikre. Det er problematisk, ikke bare fordi konklusjonene kan v√¶re metodisk feil, men fordi de i praksis kan f√∏re til at vi slutter med tiltak som faktisk har effekt.

I dette blogginnlegget viser jeg hvorfor vi ikke kan trekke slike konklusjoner ved hjelp av Monte Carlo-simuleringer. I en Monte Carlo-simulering bestemmer vi oss som forskere p√• forh√•nd for at en effekt faktisk eksisterer i populasjonen. Sp√∏rsm√•let vi deretter stiller, er ikke om effekten finnes, men hvor ofte en vanlig statistisk analyse klarer √• p√•vise den. For √• unders√∏ke dette g√•r vi frem p√• f√∏lgende m√•te: Vi lager kunstige datasett som ligner p√• det vi realistisk kunne observert i en faktisk studie. Deretter analyserer vi hvert datasett p√• samme m√•te som i ekte forskning, og registrerer om testen p√•viser effekten eller ikke. Ved √• gjenta denne prosessen mange ganger kan vi telle hvor ofte vi treffer, og hvor ofte vi bommer, selv om vi vet at effekten faktisk finnes. Denne fremgangsm√•ten gj√∏r det mulig √• skille tydelig mellom to sp√∏rsm√•l som ofte blandes sammen i forskning: om en effekt eksisterer, og om vi har tilstrekkelig statistisk styrke til √• oppdage den.

For √• gi leseren et god innsikt og overblikk g√•r jeg frem i korte, enkle steg. F√∏rst beskriver jeg et realistisk studieoppsett fra styrketrening og spesifiserer hvilken effekt det faktisk er interessant √• studere. Deretter simulerer jeg √©n enkelt studie for √• vise hvordan dataene kan se ut i praksis. Jeg bygger s√• en statistisk modell for √• teste om effekten kan p√•vises. Poenget er imidlertid ikke resultatet fra denne ene testen i denne ene studien. Hovedpoenget kommer f√∏rst n√•r vi gjentar den samme studien mange ganger. Ved √• simulere identiske studier under de samme forutsetningene, men med nye datasett hver gang, kan vi unders√∏ke hvor ofte analysen faktisk klarer √• p√•vise effekten. Til slutt varierer jeg antall deltakere per gruppe for √• vise hvordan utvalgsst√∏rrelse alene p√•virker sannsynligheten for √• trekke riktige konklusjoner, og hvorfor dette er et grunnleggende problem i store deler av idrettsforskningen.

Det er verdt √• merke seg at dette er den lange veien til poenget. I utgangspunktet burde det v√¶re mulig √• ta en langt kortere vei. Den korte forklaringen er at den klassiske nullhypotesetesten ikke tester om to grupper er like. **Den tester sannsynligheten for √• observere dataene gitt at nullhypotesen om ingen effekt er sann. Dersom observasjonene avviker tilstrekkelig fra det som forventes under nullhypotesen, sier vi at resultatet er statistisk signifikant. Men dersom resultatet ikke er statistisk signifikant, betyr det ikke at gruppene er like** ‚Äì selv om mange forskere ser ut til √• tolke det slik.

Jeg har likevel valgt √• g√• den lange veien, fordi den gj√∏r det mulig √• se hvor ofte vi faktisk trekker feil konklusjon, selv n√•r vi vet at effekten eksisterer.

## Et tenkt studieoppsett

Anta at du jobber som forsker og √∏nsker √• studere effekten av et nytt styrketreningsprogram. Du har grunn til √• tro at dette programmet vil f√∏re til st√∏rre styrke√∏kning enn tradisjonell styrketrening, fordi det i st√∏rre grad utnytter sentrale cellul√¶re mekanismer for muskelhypertrofi. Akkurat hvilke mekanismer dette er, er ikke viktig for eksempelet.

For √• teste denne hypotesen sammenligner du det nye treningsprogrammet med en tradisjonell styrketreningsmetode. Studien gjennomf√∏res over en periode p√• fire uker hos moderat godt trente ut√∏vere. For enkelhets skyld velger du 1RM i benkpress som utfallsm√•l. Du forventer at begge gruppene vil forbedre styrken sin i l√∏pet av perioden. Det avgj√∏rende sp√∏rsm√•let er derfor ikke *om* styrken √∏ker, men *hvor mye mer* den √∏ker i gruppen som f√∏lger det nye treningsprogrammet. Men hvor mye st√∏rre forbedring er det rimelig √• forvente av den nye styrketreningsmetoden? Og ikke minst, hvilken effekt er det faktisk interessant √• studere, gitt at metoden potensielt krever en betydelig atferdsendring, for eksempel hardere trening eller lengre √∏kter? Dette inneb√¶rer n√∏dvendigvis en avveiing.

Dersom forskningssp√∏rsm√•let faktisk er √• teste en hypotese, er dette sp√∏rsm√•l forskeren m√• ta stilling til f√∏r studien gjennomf√∏res. Likevel er det et punkt mange forskere ikke tar tilstrekkelig alvorlig. I idrettsforskning h√∏rer man for eksempel ofte forskere si at de er interessert i ¬´alle effekter¬ª. Begrunnelsen er gjerne at hvert sekund eller hvert kilo kan v√¶re avgj√∏rende i idrett. Det gir mening i et idrettsperspektiv, men er meningsl√∏st i et forskningsperspektiv. √Ö fange opp alle effekter krever enorme ‚Äì i praksis tiln√¶rmet uendelige ‚Äì datamengder. Det er ogs√• vanlig √• lese at forskere implisitt antar store effekter av tiltaket, fordi dette gj√∏r det enklere √• forsvare sm√• utvalg. N√•r slike antakelser ikke eksplisitt diskuteres og begrunnes, kan det f√∏re til alvorlige metodiske feil.

For enkelhets skyld velger du her en ekstra √∏kning p√• 5 kg i 1RM benkpress som den minste effekten du er interessert i. Begrunnelsen er praktisk: En slik forbedring ville v√¶rt stor nok til at du eller andre faktisk hadde v√¶rt villig til √• endre treningsopplegg. St√∏rre effekter, som 10 eller 20 kg over kort tid, fremst√•r derimot som urealistiske. Du setter derfor 5 kg som den minste effekten av interesse ‚Äì p√• engelsk omtalt som Smallest Effect Size of Interest} (SESOI). Dette gj√∏r vi med f√∏lgende kode i R:

```{r}
sesoi <- 5 # smallest effect size of interest
```

N√•r den minste effektst√∏rrelsen av interesse n√• er spesifisert, melder neste sp√∏rsm√•l seg: hvor mange deltakere trenger vi for faktisk √• kunne studere denne effekten p√• en god m√•te? I et frekventistisk rammeverk handler dette om sannsynlighet for deteksjon: Hvor ofte klarer vi √• p√•vise en effekt, gitt at en sann forskjell p√• 5 kg i populasjonen? I praksis opererer man ofte med et m√•l om rundt 0,8 sannsynlighet for √• detektere en slik effekt, noe som inneb√¶rer at man aksepterer √• bomme i omtrent 20 % av tilfellene.

For √• unders√∏ke dette ved hjelp av simulering, m√• vi gj√∏re noen eksplisitte antakelser om hvordan dataene faktisk blir til. Disse antakelsene er avgj√∏rende for den statistiske styrken til testen, og bestemmer i stor grad hvor ofte vi vil klare √• p√•vise en effekt. Antakelsene som ligger til grunn for simuleringen er oppsummert i tabellen under.

```{r}
#| echo: false
#| label: tab-antakelser
#| tbl-cap: Antakelser som ligger til grunn for simuleringen


antakelser <- tibble(
  Antakelse = c(
    "Minste effekt av interesse (SESOI)",
    "Antall deltakere per gruppe",
    "Standardavvik (baseline og post)",
    "Korrelasjon mellom baseline og post",
    "Gjennomsnittlig baseline-styrke",
    "Signifikansniv√• (Œ±)"
  ),
  Verdi = c(
    "5 kg",
    "10",
    "10 kg",
    "0,6",
    "70 kg",
    "0,05"
  ),
  Begrunnelse = c(
    "Minste forbedring som er praktisk relevant",
    "Typisk utvalgsst√∏rrelse i idrettsforskning",
    "Realistisk variasjon i 1RM benkpress",
    "Moderat‚Äìh√∏y korrelasjon, gunstig for testing med Ancova",
    "Moderat godt trente ut√∏vere",
    "Standard terskel i frekventistisk testing"
  )
)

knitr::kable(antakelser)

```

Disse antakelsene definerer det som p√• engelsk kalles \textit{data-generating process}, alts√• en beskrivelse av hvordan dataene i studien blir til. Med andre ord sier de noe om hvilke typer datasett vi kan forvente √• observere, dersom studien faktisk gjennomf√∏res under disse forutsetningene. Dersom vi endrer p√• disse antakelsene, endrer vi ogs√• hvilke datasett som kan bli trukket. Mer variasjon i dataene gir mer st√∏y, f√¶rre deltakere gir mindre informasjon, og lavere korrelasjon mellom m√•let f√∏r og etter intervensjonen gj√∏r analysen mindre presis. Antakelsene i tabellen over er derfor avgj√∏rende for hvor ofte vi vil klare √• p√•vise en effekt.

Med disse forutsetningene p√• plass, kan vi implementere dem direkte i R. Som et f√∏rste steg genererer vi ett enkelt datasett, kun for √• illustrere hvordan dataene kan se ut i praksis. Poenget er ikke denne ene simuleringen, men at den gir et konkret bilde av hva vi senere skal gjenta mange ganger.

```{r}

n <- 10 # 10 deltakere per gruppe
rho <- 0.6 # Korrelasjonen mellom baseline og post. H√∏yere korrelasjon gj√∏r at pre h√•ndterer mer variasjon
sigma_pre <- 10 # SD for baseline
sigma_post <- 10 # SD for post
group_difference <- sesoi # v√•r smallest effect size of interest, som vi definerte tidligere. I en Ancova blir dette til en intercept- forskjell


# Jeg lager gruppene a og b, hvor a skal bli tradisjonell gruppe og b skal bli den nye styrketreningsgruppen
group <- rep(c("a", "b"), each = n)

# Jeg simulerer fra en multivariate normal med 70 som gjennomsnitt for begge gruppene p√• baseline
y <- MASS::mvrnorm(
  n = n*2,
  mu = c(70, 70), # I ANCOVA er forventningen at baseline-forskjellen null 
  Sigma = matrix(
    c(sigma_pre^2,
      rho*sigma_pre*sigma_post,
      rho*sigma_pre*sigma_post,
      sigma_post^2),
    nrow = 2)
)

d <- tibble(
  group = as_factor(group),
  baseline =  round(y[,1],0),
  post = round(y[,2] + 10 + ifelse(group == "b", group_difference, 0),0) # Merk at group_difference er v√•r sesoi
)

```

For √• gi et konkret bilde av hvordan dataene kan se ut i praksis, viser jeg her ett eksempel p√• et generert datasett. Tabellen under viser √©n simulert studie, gitt antakelsene i tabellen over. Det er viktig √• understreke at dette kun er ett mulig utfall. Poenget er ikke verdiene i denne tabellen i seg selv, men at de gir en intuitiv forst√•else av hvordan datastrukturen ser ut f√∏r vi gjentar studien mange ganger.

```{r}
#| echo: false
knitr::kable(d)
```

Det er ogs√• nyttig √• visualisere disse dataene. Figuren under viser de simulerte verdiene ved baseline og etter intervensjonen for de to gruppene.

```{r}
#| echo: false
#| fig-cap: "Fordeling av 1RM benkpress ved baseline og posttest i de to gruppene. Simulert data"
d_plot <- pivot_longer(d, cols = c("baseline", "post"), names_to = "time", values_to = "kg")

ggplot(d_plot, aes(x=time, y=kg, color=group)) +
  ggbeeswarm::geom_quasirandom(width=0.05, dodge.width = 0.5, size=1.4) +
  scale_color_manual(values = colorPalette, name = "Gruppe", labels=c('Tradisjonell\nmetode', 'Ny\nmetode')) +
  theme_sim

```

## Analyse av den simulerte dataen

N√• som vi har generert et eksempel p√• et datasett, er neste steg √• bygge en statistisk modell som kan beskrive dataene. Vi velger en ANCOVA-modell, der 1RM etter treningsperioden forklares av b√•de startniv√• (baseline) og hvilken treningsgruppe deltakerne tilh√∏rer. Ved √• inkludere baseline i modellen tar vi hensyn til at deltakerne starter p√• ulike styrkeniv√•er. Samtidig gj√∏r dette det mulig √• sammenligne treningsgruppene p√• en m√•te som justerer for disse forskjellene. Effekten av treningsprogrammet kommer dermed til uttrykk som forskjellen mellom gruppene, gitt samme baseline-niv√•. Formelt kan modellen skrives som:

N√• som vi har generert et eksempel p√• et datasett, er neste steg √• bygge en statistisk modell som kan beskrive dataene. Vi velger en ANCOVA-modell, der 1RM etter treningsperioden forklares av b√•de startniv√• (baseline) og hvilken treningsgruppe deltakerne tilh√∏rer.

Ved √• inkludere baseline i modellen tar vi hensyn til at deltakerne starter p√• ulike styrkeniv√•er. Samtidig gj√∏r dette det mulig √• sammenligne treningsgruppene p√• en m√•te som justerer for disse forskjellene. Effekten av treningsprogrammet kommer dermed til uttrykk som forskjellen mellom gruppene, gitt samme baseline-niv√•. Modellen vi kan

$$
\text{Post}_i = \beta_0 + \beta_1 \text{Baseline}_i + \beta_2 \text{Gruppe}_i + \varepsilon_i
$$ der $\text{Post}_i$ er 1RM i benkpress etter treningsperioden, $\text{Baseline}_i$ er 1RM ved baseline, $\text{Gruppe}_i$ angir hvilken treningsgruppe deltakeren tilh√∏rer, og $\varepsilon_i$ er et tilfeldig feilledd.

Koeffisienten $\beta_2$ er den vi er interessert i. Den representerer den baseline-justerte gjennomsnittlige forskjellen mellom gruppene. Modellen forutsetter samtidig at gruppene i utgangspunktet ikke systematisk skiller seg fra hverandre ved baseline, alts√• at eventuelle forskjeller ved start er tilfeldige og skyldes variasjon fra utvalg til utvalg. Jeg g√•r ikke n√¶rmere inn p√• matematikken bak ANCOVA her. Det har jeg lyst til √• gj√∏re i et eget blogginnlegg.

Vi tilpasser modellen til det simulerte datasettet med f√∏lgende kode i R:

```{r}

mod_a <- lm(post ~ 1 + baseline + group, data = d)
summary(mod_a) # For √• vise summary

```

I dette ene simulerte eksempelet estimerer modellen at den nye treningsmetoden i gjennomsnitt er `r abs(round(coef(mod_a)[3], 0))` kg `r if_else(round(coef(mod_a)[3], 0) > 0, "bedre", "d√•rligere")` enn den tradisjonelle styrketreningsmetoden, n√•r vi kontrollerer for baseline. Vi ser ogs√• at vi har f√•tt en p-verdi p√• `r broom::tidy(mod_a) |> dplyr::filter(term == "groupb") |> dplyr::pull(p.value) |> round(3)`. Det er imidlertid viktig √• understreke at dette resultatet kun kommer fra √©n enkelt simulert studie. Verdien i seg selv er derfor ikke s√¶rlig interessant. Det avgj√∏rende sp√∏rsm√•let er hva som skjer n√•r vi gjentar den samme analysen mange ganger under de samme forutsetningene. Det er nettopp dette vi n√• skal unders√∏ke.

## Estimere statistisk styrke

Det neste steget er √• unders√∏ke den statistiske styrken til studien. Til dette bruker vi en Monte Carlo-simulering. Sp√∏rsm√•let vi stiller oss er: Dersom den sanne effekten av det nye styrketreningsprogrammet faktisk er 5 kg i populasjonen (som vi har bestemt at den skal v√¶re), hvor ofte vil en studie med dette oppsettet klare √• p√•vise den? Vi kan ikke gjennomf√∏re det samme eksperimentet mange ganger i virkeligheten. I stedet kan vi gj√∏re det i en datamaskin ved √• simulere studien gjentatte ganger. I en Monte Carlo-simulering gj√∏r vi tre ting:

1.  Vi spesifiserer hvordan dataene genereres, gitt v√•re antakelser (se tabell over)

2.  Vi analyserer hvert datasett med den samme statistiske modellen.

3.  Vi registrerer om analysen p√•viser effekten eller ikke.

Ved √• gjenta denne prosessen mange ganger kan vi telle hvor ofte vi p√•viser effekten (hit), og hvor ofte vi ikke gj√∏r det (miss). Andelen treff gir oss et direkte m√•l p√• den statistiske styrken.

For √• gj√∏re dette i praksis lager vi f√∏rst en funksjon, kalt sim(), som gjennomf√∏rer √©n full simulering av studien: den genererer data, tilpasser ANCOVA-modellen og returnerer resultatet. Funksjonen tar antall deltakere per gruppe, n, som argument. P√• denne m√•ten kan vi senere unders√∏ke hvordan utvalgsst√∏rrelsen p√•virker sannsynligheten for √• p√•vise en effekt.

```{r}

sim <- function(n=10) {
  n <- n # 10 deltakere per gruppe
  rho <- 0.6 # Korrelasjonen mellom baseline og post. H√∏yere korrelasjon gj√∏r at pre h√•ndterer mer variasjon
  sigma_pre <- 10 # SD for baseline
  sigma_post <- 10 # SD for post
  group_difference <- 5 # gruppeforskjellen, som i en Ancova blir intercept-forskjellen
  
  
  # Jeg lager gruppene a og b, hvor a skal bli tradisjonell gruppe og b skal bli den nye styrketreningsgruppen
  group <- rep(c("a", "b"), each = n)
  
  # Jeg simulerer fra en multivariate normal med 70 som gjennomsnitt for begge gruppene p√• baseline
  y <- MASS::mvrnorm(
    n = n*2,
    mu = c(70, 70), # I ANCOVA er forventningen at baseline-forskjellen null 
    Sigma = matrix(
      c(sigma_pre^2,
        rho*sigma_pre*sigma_post,
        rho*sigma_pre*sigma_post,
        sigma_post^2),
      nrow = 2)
  )
  
  d <- tibble(
    group = as_factor(group),
    baseline =  round(y[,1],0),
    post = round(y[,2] + 10 + ifelse(group == "b", group_difference, 0),0)
  )
  
  model <- lm(post ~ 1 + baseline + group, data = d)
  
  broom.mixed::tidy(model)
  
}

```

Vi kan n√• bruke denne funksjonen til √• simulere studien mange ganger. Som et f√∏rste eksempel genererer vi 1000 simulerte studier med 10 deltakere per gruppe.

```{r}
#| cache: true
sim_10 <- purrr::map_df(1:1e3, ~sim(n=10))
```

Dette gir oss 1000 simulerte studier. For hver simulering tilpasses den samme statistiske modellen (ANCOVA), og vi f√•r et estimat for gruppeforskjellen samt en tilh√∏rende p-verdi. Tabellen under viser 10 slike resultater som illustrasjon.

```{r}
#| echo: false
sim_10_f <- filter(sim_10, term == "groupb")
knitr::kable(head(sim_10_f, 10))
```

Med disse resultatene kan vi telle hvor ofte vi treffer (hit) og hvor ofte vi bommer (miss). For √• gj√∏re dette m√• vi f√∏rst fastsette et beslutningskriterium ‚Äì alts√• n√•r vi skal si at analysen har p√•vist en effekt. Dette kriteriet er p-verdien, og i forskning brukes det vanligvis en terskel p√• Œ±=0,05. Dersom p-verdien er lavere enn 0,05, regner vi det som at testen har p√•vist en effekt. Hvis p-verdien er h√∏yere enn 0,05, har testen ikke p√•vist effekten. P√• denne m√•ten kan vi telle hvor mange ganger analysen lykkes i √• p√•vise effekten (hit), og hvor mange ganger den ikke gj√∏r det (miss). F√∏lgende kode i R implementerer denne inndelingen:

```{r}
alpha <- 0.05

sim_10_f <- mutate(sim_10_f, detect = if_else(p.value < alpha, "hit", "miss"))


```

Tabellen under viser de 10 f√∏rste resultatene fra simuleringen, kun som illustrasjon. Hver rad tilsvarer √©n simulert studie, der vi har estimert gruppeforskjellen og avgjort om testen p√•viser effekten (hit) eller ikke (miss), gitt beslutningskriterietùõº= 0,05.

```{r}
#| echo: false
#| label: tab-hit-miss-eksempel
#| tbl-cap: Eksempel p√• 10 simulerte studier og deres utfall

knitr::kable(head(sim_10_f, 10))

```

Vi kan n√• beregne andelen ganger analysen f√∏rer til en korrekt beslutning, alts√• hvor ofte testen faktisk p√•viser effekten n√•r den finnes. Dette gir oss et direkte m√•l p√• den statistiske styrken til studien. Figuren under viser utfallet av 1000 simulerte studier med 10 deltakere per gruppe. Hver simulering klassifiseres enten som hit (effekten p√•vises) eller miss (effekten p√•vises ikke), gitt beslutningskriteriet Œ±=0,005.

```{r}
#| label: fig-hit-miss-n10
#| fig-cap: "Andel studier der effekten p√•vises (hit) og ikke p√•vises (miss), basert p√• 1000 simulerte studier med 10 deltakere per gruppe"

trials <- 1e3 # 1000 simuleringer

sim_10_f_count <- sim_10_f |>
  group_by(detect) |>
  summarise(prop = (n() / trials )) |>
  ungroup()


ggplot(sim_10_f_count, aes(x=detect, y=prop, fill=detect)) +
  geom_col() +
  scale_fill_manual(values = colorPalette, name = "Utfall av testen", labels=c('Hit', 'Miss')) +
   scale_x_discrete(labels = c(hit = "Hit", miss = "Miss")) +
  scale_y_continuous(limits=c(0, 1), expand = expansion(mult = c(0, 0))) +
  labs(x="Utfall av testen", y= "Andel detekterte effekter") +
  theme_sim

```

Dette er ikke spesielt lovende. Figuren viser at analysen bare klarer √• p√•vise effekten i omtrent `r round(sim_10_f_count |> dplyr::filter(detect == "hit") |> dplyr::pull(prop) * 100, 1)` % av tilfellene, selv om vi vet at effekten faktisk eksisterer. Med andre ord: I flertallet av studiene vil vi ikke klare √• oppdage en reell effekt med dette studieoppsettet. Dette er s√¶rlig problematisk n√•r vi vet at mange studier i idrettsforskning har rundt 10 deltakere per gruppe. Med s√• sm√• utvalg er sannsynligheten for √• p√•vise en realistisk og interessant effekt sv√¶rt lav.

For √• unders√∏ke hvordan dette bildet endrer seg med st√∏rre utvalg, gjentar vi n√• den samme prosessen med flere deltakere per gruppe. Vi sammenstiller resultatene for ulike utvalgsst√∏rrelser i √©n og samme figur, slik at vi kan se hvordan sannsynligheten for √• p√•vise effekten avhenger av antall deltakere.

```{r}
#| cache: true
num_sim <- 5:1e2
sim_all <- purrr::map_df(num_sim, function(n) {
  purrr::map_df(1:1e3, function(i) {
    sim(n = n)
  }) |>
    mutate(n = n)
})

```

Figuren under viser resultatet av Monte Carlo-simuleringen for ulike utvalgsst√∏rrelser. For hvert antall deltakere per gruppe er studien simulert 1000 ganger, og hver simulering klassifiseres som enten hit (effekten p√•vises) eller miss (effekten p√•vises ikke), gitt beslutningskriterietùõº= 0,05. Andelen hit for en gitt utvalgsst√∏rrelse kan tolkes direkte som den estimerte statistiske styrken til studien.

```{r}
#| label: fig-hit-miss_power
#| echo: false
#| fig-cap: "Statistisk styrke (andel studier der effekten p√•vises) som funksjon av antall deltakere per gruppe, basert p√• 1000 simulerte studier per utvalgsst√∏rrelse"


sim_all_f <- filter(sim_all, term == "groupb")
sim_all_f <- mutate(sim_all_f, detect = if_else(p.value < alpha, "hit", "miss"))

sim_all_f_count <- sim_all_f |>
  group_by(n, detect) |>
  summarise(prop = (n() / trials )) |>
  ungroup()

sim_plot <- sim_all_f_count

sim_plot$detect <- factor(sim_plot$detect,
                           levels = c("miss", "hit"))



ggplot(sim_plot, aes(x = factor(n), y = prop, fill = detect)) +
  geom_col(width = 0.7) +
  scale_fill_manual(
    values = colorPalette,
    name = "Utfall av testen",
    labels = c(hit = "Hit", miss = "Miss")
  ) +
  scale_x_discrete(breaks = as.character(seq(10, 100, 10))) +
  scale_y_continuous(
    limits = c(0, 1),
    expand = expansion(mult = c(0, 0))
  ) +
  labs(
    x = "Antall deltakere per gruppe",
    y = "Andel detekterte effekter"
  ) +
  theme_sim

```

I figuren over ser vi tydelig at med et lavt antall deltakere per gruppe er det n√¶rmest umulig √• trekke riktige slutninger om den sanne effekten i populasjonen. Dette gjelder selv for en effekt som er b√•de realistisk og som √•penbart burde v√¶re av interesse for forskere. N√•r vi samtidig vet at studier i idrettsforskning ofte har mellom 10 og 20 deltakere per gruppe, b√∏r vi v√¶re sv√¶rt forsiktige med √• trekke skr√•sikre konklusjoner, s√¶rlig n√•r resultatet er at man ikke klarer √• p√•vise en effekt. Figuren over illustrerer tydelig forskjellen mellom at en effekt faktisk eksisterer, og at vi har tilstrekkelig statistisk styrke til √• oppdage den. Dette skillet overses altfor ofte. En effekt kan v√¶re b√•de reell og praktisk viktig, men likevel forbli uoppdaget dersom utvalget er for lite. F√∏rst n√•r utvalgsst√∏rrelsen n√¶rmer seg rundt 50 deltakere per gruppe, begynner sannsynligheten for √• p√•vise en slik effekt √• n√¶rme seg niv√•er som vanligvis regnes som akseptable i forskning. Dette er langt h√∏yere enn det som er vanlig praksis i feltet, og det setter store deler av litteraturen i et ubehagelig lys.

N√•r det er sagt er ikke sm√• studier verdil√∏se. Men de kan ikke brukes til √• avkrefte effekter de aldri hadde en reell sjanse til √• oppdage. Uten en eksplisitt definisjon av hvilken effekt man faktisk er interessert i, og uten et studiedesign som gir rimelig sannsynlighet for √• p√•vise den, er ¬´ingen effekt¬ª ikke en konklusjon, det er en logisk feilslutning.

Altfor ofte betyr ¬´ingen effekt¬ª bare √©n ting: Vi hadde ikke nok data.
